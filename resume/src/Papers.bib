@article{bouvierSolvingContinentScaleInventory2023,
  title = {Solving a {{Continent-Scale Inventory Routing Problem}} at {{Renault}}},
  author = {Bouvier, Louis and Dalle, Guillaume and Parmentier, Axel and Vidal, Thibaut},
  year = {2023},
  month = oct,
  journal = {Transportation Science},
  publisher = {{INFORMS}},
  issn = {0041-1655},
  doi = {10.1287/trsc.2022.0342},
  abbr = {Journal},
  abstract = {This paper is the fruit of a partnership with Renault. Their reverse logistic requires solving a continent-scale multiattribute inventory routing problem (IRP). With an average of 30 commodities, 16 depots, and 600 customers spread across a continent, our instances are orders of magnitude larger than those in the literature. Existing algorithms do not scale, so we propose a large neighborhood search (LNS). To make it work, (1) we generalize existing split delivery vehicle routing problems and IRP neighborhoods to this context, (2) we turn a state-of-the-art matheuristic for medium-scale IRP into a large neighborhood, and (3) we introduce two novel perturbations: the reinsertion of a customer and that of a commodity into the IRP solution. We also derive a new lower bound based on a flow relaxation. In order to stimulate the research on large-scale IRP, we introduce a library of industrial instances. We benchmark our algorithms on these instances and make our code open source. Extensive numerical experiments highlight the relevance of each component of our LNS. Funding: This work was supported by Renault Group. Supplemental Material: The online appendix is available at https://doi.org/10.1287/trsc.2022.0342.},
  arxivid = {2209.00412},
  website = {https://pubsonline.informs.org/doi/full/10.1287/trsc.2022.0342},
  keywords = {cv,paper,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/3DIVZ724/Bouvier et al. - 2023 - Solving a Continent-Scale Inventory Routing Proble.pdf}
}

@misc{clarteAnalysisBootstrapSubsampling2024,
  title = {Analysis of {{Bootstrap}} and {{Subsampling}} in {{High-dimensional Regularized Regression}}},
  author = {Clart{\'e}, Lucas and Vandenbroucque, Adrien and Dalle, Guillaume and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13622},
  eprint = {2402.13622},
  primaryclass = {cond-mat, stat},
  publisher = {{arXiv}},
  abstract = {We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples \$n\$ and dimension \$d\$ of the covariates grow at a comparable fixed rate \${\textbackslash}alpha{\textbackslash}!={\textbackslash}! n/d\$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when \${\textbackslash}alpha\$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime \${\textbackslash}alpha{\textbackslash}!{$<\backslash$}!1\$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization.},
  archiveprefix = {arxiv},
  keywords = {cv,paper,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/8AC4WH8X/ClartÃ© et al. - 2024 - Analysis of Bootstrap and Subsampling in High-dime.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/92NK285D/2402.html}
}

@misc{dalleLearningCombinatorialOptimization2022a,
  title = {Learning with {{Combinatorial Optimization Layers}}: A {{Probabilistic Approach}}},
  shorttitle = {Learning with {{Combinatorial Optimization Layers}}},
  author = {Dalle, Guillaume and Baty, L{\'e}o and Bouvier, Louis and Parmentier, Axel},
  year = {2022},
  month = dec,
  number = {arXiv:2207.13513},
  eprint = {2207.13513},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.13513},
  abbr = {Preprint},
  abstract = {Combinatorial optimization (CO) layers in machine learning (ML) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a CO problem often behaves as a piecewise constant function of its objective parameters. Given that ML pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard ML losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of CO layers. In this paper, building upon previous works, we introduce a probabilistic perspective on CO layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present InferOpt.jl, an open-source Julia package that 1) allows turning any CO oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's ML ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps as guiding example, as well as three other applications from operations research.},
  archiveprefix = {arxiv},
  arxivid = {2207.13513},
  keywords = {cv,paper,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/LHGVCG2Q/Dalle et al. - 2022 - Learning with Combinatorial Optimization Layers a.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/LLJPNBI7/2207.html}
}

@phdthesis{dalleMachineLearningCombinatorial2022,
  title = {Machine Learning and Combinatorial Optimization Algorithms, with Applications to Railway Planning},
  author = {Dalle, Guillaume},
  year = {2022},
  month = dec,
  abbr = {Dissertation},
  abstract = {This thesis investigates the frontier between machine learning and combinatorial optimization, two active areas of applied mathematics research. We combine theoretical insights with efficient algorithms, and develop several open source Julia libraries. Inspired by a collaboration with the Soci{\'e}t{\'e} nationale des chemins de fer fran{\c c}ais (SNCF), we study high-impact use cases from the railway world: train failure prediction, delay propagation, and track allocation.In Part I, we provide mathematical background and describe software implementations for various tools that will be needed later on: implicit differentiation, temporal point processes, Hidden Markov Models and Multi-Agent Path Finding. Our publicly-available code fills a void in the Julia package ecosystem, aiming at ease of use without compromising on performance.In Part II, we highlight theoretical contributions related to both statistics and decision-making. We consider a Vector AutoRegressive process with partial observations, and prove matching upper and lower bounds on the estimation error. We unify and extend the state of the art for combinatorial optimization layers in deep learning, gathering various approaches in a Julia library called InferOpt.jl. We also seek to differentiate through multi-objective optimization layers, which leads to a novel theory of lexicographic convex analysis.In Part III, these mathematical and algorithmic foundations come together to tackle railway problems. We design a hierarchical model of train failures, propose a graph-based framework for delay propagation, and suggest new avenues for track allocation, with the Flatland challenge as a testing ground.},
  collaborator = {Meunier, Fr{\'e}d{\'e}ric and De Castro, Yohann and Parmentier, Axel},
  copyright = {Licence Etalab},
  langid = {english},
  pdf = {https://pastel.archives-ouvertes.fr/tel-04053322},
  school = {{\'E}cole des Ponts ParisTech},
  keywords = {cv,hmm,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/CEVJMUP4/Dalle - Machine learning and combinatorial optimization al.pdf}
}

@misc{dalleMinimaxEstimationPartiallyObserved2022,
  title = {Minimax {{Estimation}} of {{Partially-Observed Vector AutoRegressions}}},
  author = {Dalle, Guillaume and {de Castro}, Yohann},
  year = {2022},
  month = may,
  number = {arXiv:2106.09327},
  eprint = {2106.09327},
  primaryclass = {eess, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09327},
  abbr = {Preprint},
  abstract = {High-dimensional time series are a core ingredient of the statistical modeling toolkit, for which numerous estimation methods are known.But when observations are scarce or corrupted, the learning task becomes much harder.The question is: how much harder? In this paper, we study the properties of a partially-observed Vector AutoRegressive process, which is a state-space model endowed with a stochastic observation mechanism.Our goal is to estimate its sparse transition matrix, but we only have access to a small and noisy subsample of the state components.Interestingly, the sampling process itself is random and can exhibit temporal correlations, a feature shared by many realistic data acquisition scenarios.We start by describing an estimator based on the Yule-Walker equation and the Dantzig selector, and we give an upper bound on its non-asymptotic error.Then, we provide a matching minimax lower bound, thus proving near-optimality of our estimator.The convergence rate we obtain sheds light on the role of several key parameters such as the sampling ratio, the amount of noise and the number of non-zero coefficients in the transition matrix.These theoretical findings are commented and illustrated by numerical experiments on simulated data.},
  archiveprefix = {arxiv},
  arxivid = {2106.09327},
  keywords = {cv,paper,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/DV273BGK/Dalle and de Castro - 2022 - Minimax Estimation of Partially-Observed Vector Au.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/KB633BG9/2106.html}
}

@patent{stephanMethodDeterminingSoiling2020,
  title = {Method for Determining a Soiling Speed of a Photovoltaic Generation Unit},
  author = {Stephan, Pierre and Dalle, Guillaume},
  year = {2020},
  month = jun,
  number = {WO2020115431A1},
  abbr = {Patent},
  abstract = {The invention relates to a method for determining a soiling speed of a photovoltaic generation unit, in which, on the basis of values of an electrical variable generated by the photovoltaic generation unit at a plurality of moments in a time series and corresponding values of meteorological parameters, and on the basis of a relationship between an electrical variable generated by said generation unit at one moment, the values taken with the meteorological parameters at the same moment, and the occurrence of cleaning events, wherein said relationship comprises multiple relational parameters including the speed of soiling and the occurrence of a cleaning event is modelled by a probability law involving a relational parameter, the soiling speed is determined in iterations in which vectors that are representative of the occurrence of cleaning events are simulated.},
  assignee = {Electricite De France},
  langid = {english},
  nationality = {WO},
  website = {https://patents.google.com/patent/WO2020115431A1/en},
  keywords = {cv,website},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/UD56J2RP/Stephan_Dalle_2020_Method for determining a soiling speed of a photovoltaic generation unit.pdf}
}
